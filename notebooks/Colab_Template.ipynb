{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07782167",
   "metadata": {},
   "source": [
    "# AI-Powered Fracture Detection — Colab Template\n",
    "Team: Shashank, Suprita, Srujan\n",
    "\n",
    "This notebook contains a step-by-step Colab-ready template for dataset download, preprocessing, training (transfer learning), evaluation, and Grad-CAM visualizations. Follow cells in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc4fbe",
   "metadata": {},
   "source": [
    "## 1) Notebook configuration & reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895377ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project metadata and reproducibility settings\n",
    "TEAM = ['Shashank', 'Suprita', 'Srujan']\n",
    "DURATION_WEEKS = 6\n",
    "DATA_DIR = '/content/data'  # adjust if mounting Drive\n",
    "SEED = 42\n",
    "import random, numpy as np, tensorflow as tf\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "print('Seeds set, TF version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52e6f5",
   "metadata": {},
   "source": [
    "## 2) Environment & dependencies (Colab friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd46d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install extras if running in a fresh Colab session (comment out locally)\n",
    "!pip install -q tensorflow albumentations opencv-python scikit-image scikit-learn \n",
    "import tensorflow as tf\n",
    "print('GPU devices:', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c371a",
   "metadata": {},
   "source": [
    "## 3) Dataset download & folder layout (instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: create canonical folders\n",
    "from pathlib import Path\n",
    "base = Path('/content/data')\n",
    "for split in ['train','val','test']:\n",
    "    for cls in ['fracture','normal']:\n",
    "        (base/split/cls).mkdir(parents=True, exist_ok=True)\n",
    "print(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be799a9",
   "metadata": {},
   "source": [
    "## 4) Exploratory Data Analysis (samples & counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e890c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "base = Path('/content/data')\n",
    "for split in ['train','val','test']:\n",
    "    counts = {cls: len(list((base/split/cls).glob('*'))) for cls in ['fracture','normal']}\n",
    "    print(split, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3c9fb",
   "metadata": {},
   "source": [
    "## 5) Preprocessing utilities (resize, normalize, to-RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa44a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "def load_image(path, target_size=(224,224)):\n",
    "    img = Image.open(path)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size)\n",
    "    arr = np.array(img)/255.0\n",
    "    return arr\n",
    "# Example usage (replace with real file path)\n",
    "# sample = load_image('/content/data/train/normal/example.jpg')\n",
    "# print(sample.shape, sample.min(), sample.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce83f5a",
   "metadata": {},
   "source": [
    "## 6) Augmentation examples (Albumentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "transform = A.Compose([\n",
    "    A.Rotate(limit=15, p=0.7),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "])\n",
    "# example application\n",
    "# aug = transform(image=(sample*255).astype('uint8'))['image']\n",
    "# plt.imshow(aug); plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19737583",
   "metadata": {},
   "source": [
    "## 7) tf.data loader (efficient pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80486283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def make_dataset(file_paths, labels, batch_size=16, image_size=(224,224), training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    def _load(path, label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.io.decode_image(img, channels=3)\n",
    "        img = tf.image.resize(img, image_size)\n",
    "        img = img / 255.0\n",
    "        return img, label\n",
    "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(1024)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "print('tf.data loader ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b53b6",
   "metadata": {},
   "source": [
    "## 8) Baseline custom CNN (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "def small_cnn(input_shape=(224,224,3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32,3,activation='relu')(inputs)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.Conv2D(64,3,activation='relu')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "m = small_cnn()\n",
    "m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6022e5",
   "metadata": {},
   "source": [
    "## 9) Transfer learning example (ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2aeff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "base = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "base.trainable = False\n",
    "x = layers.GlobalAveragePooling2D()(base.output)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "out = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = models.Model(inputs=base.input, outputs=out)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de461f4",
   "metadata": {},
   "source": [
    "## 10) Training loop & callbacks (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02583f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "ckpt = ModelCheckpoint('/content/best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "rlr = ReduceLROnPlateau(patience=3)\n",
    "# history = model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=[ckpt, es, rlr])\n",
    "print('Callbacks ready — run model.fit with your datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f417b9",
   "metadata": {},
   "source": [
    "## 11) Evaluation (metrics & confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111da533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "# preds = model.predict(test_ds)  # probabilities\n",
    "# y_true = ... ; y_pred = (preds >= 0.5).astype(int)\n",
    "# print(classification_report(y_true, y_pred))\n",
    "print('Use sklearn.metrics for evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436d944",
   "metadata": {},
   "source": [
    "## 12) Grad-CAM (placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for Grad-CAM utilities — implement grad_cam(model, img_tensor) to visualize attention\n",
    "print('Implement Grad-CAM as a utility function and apply to misclassified samples')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
